{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "about-operation",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg\n",
    "if isfile(\"../Project.toml\") && isfile(\"../Manifest.toml\")\n",
    "    Pkg.activate(\"..\");\n",
    "    ENV[\"PYTHON\"] = \"python3\";\n",
    "end\n",
    "\n",
    "using JuliaProbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "associate-numbers",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dt = 0.1\n",
    "# environment\n",
    "xlim = [-5.0, 5.0]\n",
    "ylim = [-5.0, 5.0]\n",
    "# id of landmark must start from 0 with 1 step\n",
    "landmarks = [\n",
    "    Landmark([-4.0, 2.0], 0),\n",
    "    Landmark([2.0, -3.0], 1),\n",
    "    Landmark([4.0, 4.0], 2),\n",
    "    Landmark([-4.0, -4.0], 3)\n",
    "]\n",
    "envmap = Map()\n",
    "push!(envmap, landmarks)\n",
    "world = PuddleWorld(xlim, ylim)\n",
    "push!(world, envmap)\n",
    "# goal\n",
    "goal = Goal(-3.0, -3.0)\n",
    "push!(world, goal)\n",
    "# robot side\n",
    "initial_pose = [2.0, 2.0, 0.0]\n",
    "estimator = KalmanFilter(envmap, initial_pose)\n",
    "# this resolution must be consistant with the resolution\n",
    "# with which policy_amdp.txt was generated in BeliefDP.\n",
    "reso = [0.2, 0.2, pi / 18]\n",
    "dp_agent = AMDPPolicyAgent(0.2, 10.0 * pi / 180, dt, estimator, goal, reso)\n",
    "sampling_num = 10\n",
    "init_policy(dp_agent, \"policy_amdp.txt\")\n",
    "\n",
    "robot = RealRobot(initial_pose, dp_agent, RealCamera(landmarks); color = \"red\")\n",
    "push!(world, robot)\n",
    "# puddles\n",
    "push!(world, Puddle([-2.0, 0.0], [0.0, 2.0], 0.1))\n",
    "push!(world, Puddle([-0.5, -2.0], [2.5, 1.0], 0.1))\n",
    "\n",
    "anim = @animate for i = 1:130\n",
    "    t = dt * i\n",
    "    annota = \"t = $(round(t, sigdigits=3))[s]\"\n",
    "    # t\n",
    "    update_status(world)\n",
    "    z = observations(robot.sensor_, robot.pose_; noise = true, bias = true)\n",
    "    p = draw(world, annota)\n",
    "\n",
    "    # t+1\n",
    "    v, ω = decision(dp_agent, z)\n",
    "    # v, ω = policy(dp_agent, agent.estimator_.pose_)\n",
    "    state_transition(robot, v, ω, dt; move_noise = true, vel_bias_noise = true)\n",
    "end\n",
    "gif(anim, \"images/ch12_amdp_policy_agent.gif\", fps = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shared-discharge",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = 0.1\n",
    "# environment\n",
    "xlim = [-5.0, 5.0]\n",
    "ylim = [-5.0, 5.0]\n",
    "# id of landmark must start from 0 with 1 step\n",
    "landmarks = [\n",
    "    Landmark([-4.0, 2.0], 0),\n",
    "    Landmark([2.0, -3.0], 1),\n",
    "    Landmark([4.0, 4.0], 2),\n",
    "    Landmark([-4.0, -4.0], 3)\n",
    "]\n",
    "envmap = Map()\n",
    "push!(envmap, landmarks)\n",
    "world = PuddleWorld(xlim, ylim)\n",
    "push!(world, envmap)\n",
    "# goal\n",
    "goal = Goal(-3.0, -3.0)\n",
    "push!(world, goal)\n",
    "# robot side\n",
    "initial_pose = [2.0, 2.0, 0.0]\n",
    "estimator = KalmanFilter(envmap, initial_pose)\n",
    "# this resolution must be consistant with the resolution\n",
    "# with which policy_amdp.txt was generated in BeliefDP.\n",
    "reso = [0.2, 0.2, pi / 18]\n",
    "dp_agent = AMDPPolicyAgent(0.2, 10.0 * pi / 180, dt, estimator, goal, reso)\n",
    "sampling_num = 10\n",
    "init_policy(dp_agent, \"policy_amdp2.txt\")\n",
    "\n",
    "robot = RealRobot(initial_pose, dp_agent, RealCamera(landmarks); color = \"red\")\n",
    "push!(world, robot)\n",
    "# puddles\n",
    "push!(world, Puddle([-2.0, 0.0], [0.0, 2.0], 0.1))\n",
    "push!(world, Puddle([-0.5, -2.0], [2.5, 1.0], 0.1))\n",
    "\n",
    "anim = @animate for i = 1:200\n",
    "    t = dt * i\n",
    "    annota = \"t = $(round(t, sigdigits=3))[s]\"\n",
    "    # t\n",
    "    update_status(world)\n",
    "    z = observations(robot.sensor_, robot.pose_; noise = true, bias = true)\n",
    "    p = draw(world, annota)\n",
    "\n",
    "    # t+1\n",
    "    v, ω = decision(dp_agent, z)\n",
    "    # v, ω = policy(dp_agent, agent.estimator_.pose_)\n",
    "    state_transition(robot, v, ω, dt; move_noise = true, vel_bias_noise = true)\n",
    "end\n",
    "gif(anim, \"images/ch12_amdp_policy_agent2.gif\", fps = 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.0",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
