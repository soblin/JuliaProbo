{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "about-operation",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg\n",
    "if isfile(\"../Project.toml\") && isfile(\"../Manifest.toml\")\n",
    "    Pkg.activate(\"..\");\n",
    "    ENV[\"PYTHON\"] = \"python3\";\n",
    "end\n",
    "\n",
    "using JuliaProbo\n",
    "using Plots\n",
    "gr();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "social-tissue",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlim = [-5.0, 5.0]\n",
    "ylim = [-5.0, 5.0]\n",
    "world = PuddleWorld(xlim, ylim)\n",
    "push!(world, Puddle([-2.0, 0.0], [0.0, 2.0], 0.1))\n",
    "push!(world, Puddle([-0.5, -2.0], [2.5, 1.0], 0.1))\n",
    "sampling_num = 10\n",
    "dp = DynamicProgramming([0.1, 0.1, pi/20], Goal(-3.0, -3.0), dt=0.1);\n",
    "init_value(dp)\n",
    "init_policy(dp)\n",
    "init_state_transition_probs(dp, 0.1, sampling_num)\n",
    "init_depth(dp, world, sampling_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portable-prisoner",
   "metadata": {},
   "outputs": [],
   "source": [
    "Δ = 1e100\n",
    "sweep_num = 0\n",
    "while Δ > 0.01 && sweep_num < 300\n",
    "    Δ = value_iteration_sweep(dp)\n",
    "    sweep_num += 1\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verified-biography",
   "metadata": {},
   "outputs": [],
   "source": [
    "fd = open(\"policy.txt\", \"w\")\n",
    "for index in dp.indices\n",
    "    v, ω = dp.policy_[index..., :]\n",
    "    write(fd, \"$(index[1]) $(index[2]) $(index[3]) $(v) $(ω)\\n\")\n",
    "end\n",
    "close(fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "returning-literature",
   "metadata": {},
   "outputs": [],
   "source": [
    "import JuliaProbo: init_policy, policy, decision, draw, PuddleIgnoreAgent, observation_update, reward_per_sec, motion_update\n",
    "import Plots: Plot\n",
    "\n",
    "mutable struct DpPolicyAgent <: AbstractAgent\n",
    "    puddle_ignore_agent_::PuddleIgnoreAgent\n",
    "    reso::Vector{Float64}\n",
    "    pose_min::Vector{Float64}\n",
    "    pose_max::Vector{Float64}\n",
    "    index_nums::Vector{Int64}\n",
    "    policy_data::AbstractArray{Float64,4}\n",
    "end\n",
    "\n",
    "function DpPolicyAgent(agent::PuddleIgnoreAgent, reso::Vector{Float64}; lowerleft = [-4.0, -4.0], upperright = [4.0, 4.0])\n",
    "    pose_min = vcat(lowerleft, [0.0])\n",
    "    pose_max = vcat(upperright, [2pi])\n",
    "    index_nums = [convert(Int64, round((pose_max[i] - pose_min[i]) / reso[i])) for i = 1:3]\n",
    "    policy_data = zeros(Float64, index_nums..., 2)\n",
    "    fd = open(\"policy.txt\", \"r\")\n",
    "    lines = readlines(fd)\n",
    "    for line in lines\n",
    "        tokens = split(line, \" \")\n",
    "        tokens = map(x -> parse(Float64, x), tokens)\n",
    "        id1, id2, id3 = map(x -> convert(Int64, x), tokens[1:3])\n",
    "        v, ω = tokens[4], tokens[5]\n",
    "        policy_data[id1, id2, id3, :] = [v, ω]\n",
    "    end\n",
    "    return DpPolicyAgent(agent, reso, pose_min, pose_max, index_nums, policy_data)\n",
    "end\n",
    "\n",
    "function policy(agent::DpPolicyAgent, pose::Vector{Float64}, goal = nothing)\n",
    "    if agent.puddle_ignore_agent_.in_goal_\n",
    "        return [0.0, 0.0]\n",
    "    end\n",
    "    reso = agent.reso\n",
    "    pose_min = agent.pose_min\n",
    "    index_nums = agent.index_nums\n",
    "    index = [convert(Int64, round((pose[i] - pose_min[i]) / reso[i])) for i = 1:3]\n",
    "    index[3] = (index[3] + 10*index_nums[3]) % index_nums[3]\n",
    "    if index[3] == 0\n",
    "        index[3] = index_nums[3]\n",
    "    end\n",
    "    for i in [1, 2]\n",
    "        if index[i] < 0\n",
    "            index[i] = 0\n",
    "        end\n",
    "        if index[i] > index_nums[i]\n",
    "            index[i] = index_nums[i]\n",
    "        end\n",
    "    end\n",
    "    return agent.policy_data[index..., :]\n",
    "end\n",
    "\n",
    "function decision(dp_agent::DpPolicyAgent, observation::Vector{Vector{Float64}}, envmap = Map(); kwargs...)\n",
    "    agent = dp_agent.puddle_ignore_agent_\n",
    "    if agent.in_goal_\n",
    "        return 0.0, 0.0\n",
    "    end\n",
    "    motion_update(agent.estimator_, agent.prev_v_, agent.prev_ω_, agent.dt)\n",
    "    observation_update(agent.estimator_, observation, envmap; kwargs...)\n",
    "    agent.total_reward_ += agent.dt * reward_per_sec(agent)\n",
    "\n",
    "    v, ω = policy(dp_agent, agent.estimator_.pose_, agent.goal)\n",
    "    agent.prev_v_, agent.prev_ω_ = v, ω\n",
    "    return v, ω\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "associate-numbers",
   "metadata": {},
   "outputs": [],
   "source": [
    "function ch12_dp_kf()\n",
    "    dt = 0.1\n",
    "    # environment\n",
    "    xlim = [-5.0, 5.0]\n",
    "    ylim = [-5.0, 5.0]\n",
    "    # id of landmark must start from 0 with 1 step\n",
    "    landmarks =\n",
    "        [Landmark([-4.0, 2.0], 0), Landmark([2.0, -3.0], 1), Landmark([4.0, 4.0], 2),  Landmark([-4.0, -4.0], 3)]\n",
    "    envmap = Map()\n",
    "    push!(envmap, landmarks)\n",
    "    world = PuddleWorld(xlim, ylim)\n",
    "    push!(world, envmap)\n",
    "    # goal\n",
    "    goal = Goal(-3.0, -3.0)\n",
    "    push!(world, goal)\n",
    "    # robot side\n",
    "    initial_pose = [2.0, 2.0, 0.0]\n",
    "    estimator = KalmanFilter(envmap, initial_pose)\n",
    "    agent = PuddleIgnoreAgent(0.2, 10.0 * pi / 180, dt, estimator, goal)\n",
    "    reso = [0.1, 0.1, pi / 20]\n",
    "    dp_agent = DpPolicyAgent(agent, reso)\n",
    "    robot = RealRobot(initial_pose, agent, RealCamera(landmarks); color = \"red\")\n",
    "    push!(world, robot)\n",
    "    # puddles\n",
    "    push!(world, Puddle([-2.0, 0.0], [0.0, 2.0], 0.1))\n",
    "    push!(world, Puddle([-0.5, -2.0], [2.5, 1.0], 0.1))\n",
    "\n",
    "    anim = @animate for i = 1:180\n",
    "        t = dt * i\n",
    "        annota = \"t = $(round(t, sigdigits=3))[s]\"\n",
    "        # t\n",
    "        update_status(world)\n",
    "        z = observations(robot.sensor_, robot.pose_; noise = true, bias = true)\n",
    "        p = draw(world, annota)\n",
    "        \n",
    "        # t+1\n",
    "        v, ω = decision(dp_agent, z)\n",
    "        # v, ω = policy(dp_agent, agent.estimator_.pose_)\n",
    "        state_transition(robot, v, ω, dt; move_noise = true, vel_bias_noise = true)\n",
    "    end\n",
    "    gif(anim, \"images/ch12_dp_kf.gif\", fps = 10)\n",
    "end\n",
    "ch12_dp_kf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interior-chaos",
   "metadata": {},
   "outputs": [],
   "source": [
    "function ch12_dp_mcl()\n",
    "    dt = 0.1\n",
    "    # environment\n",
    "    xlim = [-5.0, 5.0]\n",
    "    ylim = [-5.0, 5.0]\n",
    "    # id of landmark must start from 0 with 1 step\n",
    "    landmarks =\n",
    "        [Landmark([-4.0, 2.0], 0), Landmark([2.0, -3.0], 1), Landmark([4.0, 4.0], 2),  Landmark([-4.0, -4.0], 3)]\n",
    "    envmap = Map()\n",
    "    push!(envmap, landmarks)\n",
    "    world = PuddleWorld(xlim, ylim)\n",
    "    push!(world, envmap)\n",
    "    # goal\n",
    "    goal = Goal(-3.0, -3.0)\n",
    "    push!(world, goal)\n",
    "    # robot side\n",
    "    initial_pose = [2.0, 2.0, 0.0]\n",
    "    # estimator = KalmanFilter(envmap, initial_pose)\n",
    "    estimator = Mcl(initial_pose, 100)\n",
    "    agent = PuddleIgnoreAgent(0.2, 10.0 * pi / 180, dt, estimator, goal)\n",
    "    reso = [0.1, 0.1, pi / 20]\n",
    "    dp_agent = DpPolicyAgent(agent, reso)\n",
    "    robot = RealRobot(initial_pose, agent, RealCamera(landmarks); color = \"red\")\n",
    "    push!(world, robot)\n",
    "    # puddles\n",
    "    push!(world, Puddle([-2.0, 0.0], [0.0, 2.0], 0.1))\n",
    "    push!(world, Puddle([-0.5, -2.0], [2.5, 1.0], 0.1))\n",
    "\n",
    "    anim = @animate for i = 1:180\n",
    "        t = dt * i\n",
    "        annota = \"t = $(round(t, sigdigits=3))[s]\"\n",
    "        # t\n",
    "        update_status(world)\n",
    "        z = observations(robot.sensor_, robot.pose_; noise = true, bias = true)\n",
    "        p = draw(world, annota)\n",
    "        \n",
    "        # t+1\n",
    "        v, ω = decision(dp_agent, z, envmap; resample = true)\n",
    "        # v, ω = policy(dp_agent, agent.estimator_.pose_)\n",
    "        state_transition(robot, v, ω, dt; move_noise = true, vel_bias_noise = true)\n",
    "    end\n",
    "    gif(anim, \"images/ch12_dp_mcl.gif\", fps = 10)\n",
    "end\n",
    "ch12_dp_mcl()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conditional-obligation",
   "metadata": {},
   "source": [
    "<img src=\"images/ch12_dp_kf.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flush-utility",
   "metadata": {},
   "source": [
    "<img src=\"images/ch12_dp_mcl.gif\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.0",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
